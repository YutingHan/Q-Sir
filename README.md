# Q-Sir
Weblink: https://yutinghan.github.io/Q-Sir/

Video: Q-Sir/Presentation.mp4

## HomePage:
![HomePage](/media/homePage.png)


## ResultPge: 
![ResultPage](/media/resultPage.gif)


## Model: 
![Model](/png/3.gif)
BERT relies on transformers. A basic Transformer consists of an encoder that reads text input and a decoder that predicts the task. Since the goal of BERT is to generate language representation models, only the encoder part is needed. The transformer encoder reads the entire word sequence at once. This is the opposite of the previous work, which is to view the text sequence from left to right or from left to right and right to left.

XLNet is an automatic regression language model based on recursive converter architecture, which outputs the joint probability of token sequences. Its training target calculates the probability of word tags. This condition depends on all permutations of word tags in a sentence, which is the opposite of tags that are only on the left or right of the target tag.


Technical Achievements
------
- 1
- 2


Design Achievements
------
- 1
- 2

